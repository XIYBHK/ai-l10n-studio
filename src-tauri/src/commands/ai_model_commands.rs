use crate::services::ai::{CostBreakdown, CostCalculator, ModelInfo, ProviderInfo};
use crate::services::ai::provider::with_global_registry;

/// è·å–æŒ‡å®šä¾›åº”å•†çš„æ‰€æœ‰æ¨¡å‹
///
/// ç¤ºä¾‹ï¼š
/// ```ts
/// const models = await invoke<ModelInfo[]>('get_provider_models', { providerId: 'openai' });
/// ```
#[tauri::command]
pub fn get_provider_models(provider_id: String) -> Result<Vec<ModelInfo>, String> {
    with_global_registry(|registry| {
        // ğŸ”§ è°ƒè¯•æ—¥å¿—ï¼šè®°å½•å½“å‰è¯·æ±‚å’Œæ³¨å†Œè¡¨çŠ¶æ€
        let all_ids = registry.get_provider_ids();
        log::info!("ğŸ” è¯·æ±‚ä¾›åº”å•†æ¨¡å‹: provider_id={}", provider_id);
        log::info!("ğŸ” å½“å‰æ³¨å†Œçš„ä¾›åº”å•†: {:?}", all_ids);
        
        registry.get_provider(&provider_id)
            .map(|provider| {
                let models = provider.get_models();
                log::info!("âœ… æ‰¾åˆ°ä¾›åº”å•† '{}', è¿”å› {} ä¸ªæ¨¡å‹", provider_id, models.len());
                models
            })
            .ok_or_else(|| {
                let error_msg = format!("æœªæ‰¾åˆ°ä¾›åº”å•†: '{}' (å¯ç”¨: {:?})", provider_id, all_ids);
                log::error!("âŒ {}", error_msg);
                error_msg
            })
    })
}

/// è·å–æŒ‡å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
///
/// ç¤ºä¾‹ï¼š
/// ```ts
/// const model = await invoke<ModelInfo>('get_model_info', {
///   providerId: 'openai',
///   modelId: 'gpt-4o-mini'
/// });
/// ```
#[tauri::command]
pub fn get_model_info(provider_id: String, model_id: String) -> Result<Option<ModelInfo>, String> {
    with_global_registry(|registry| {
        if let Some(provider) = registry.get_provider(&provider_id) {
            Ok(provider.get_model_info(&model_id))
        } else {
            Err(format!("æœªæ‰¾åˆ°ä¾›åº”å•†: {}", provider_id))
        }
    })
}

/// ä¼°ç®—ç¿»è¯‘æˆæœ¬
///
/// åŸºäºå­—ç¬¦æ•°ä¼°ç®—æ‰¹é‡ç¿»è¯‘çš„æˆæœ¬
///
/// å‚æ•°ï¼š
/// - `provider_id`: ä¾›åº”å•†ID
/// - `model_id`: æ¨¡å‹ID
/// - `char_count`: å­—ç¬¦æ•°
/// - `cache_hit_rate`: å¯é€‰çš„ç¼“å­˜å‘½ä¸­ç‡ï¼ˆ0.0-1.0ï¼Œé»˜è®¤0.3ï¼‰
///
/// ç¤ºä¾‹ï¼š
/// ```ts
/// const cost = await invoke<number>('estimate_translation_cost', {
///   providerId: 'openai',
///   modelId: 'gpt-4o-mini',
///   charCount: 10000,
///   cacheHitRate: 0.3
/// });
/// ```
#[tauri::command]
pub fn estimate_translation_cost(
    provider_id: String,
    model_id: String,
    char_count: usize,
    cache_hit_rate: Option<f64>,
) -> Result<f64, String> {
    let model = with_global_registry(|registry| {
        registry.get_provider(&provider_id)
            .and_then(|provider| provider.get_model_info(&model_id))
            .ok_or_else(|| format!("æœªæ‰¾åˆ°æ¨¡å‹: {} (ä¾›åº”å•†: {})", model_id, provider_id))
    })?;

    let hit_rate = cache_hit_rate.unwrap_or(0.3); // é»˜è®¤30%ç¼“å­˜å‘½ä¸­ç‡

    // éªŒè¯ç¼“å­˜å‘½ä¸­ç‡èŒƒå›´
    if !(0.0..=1.0).contains(&hit_rate) {
        return Err("ç¼“å­˜å‘½ä¸­ç‡å¿…é¡»åœ¨ 0.0-1.0 ä¹‹é—´".to_string());
    }

    let cost = CostCalculator::estimate_batch_cost(&model, char_count, hit_rate);

    Ok(cost)
}

/// è®¡ç®—ç²¾ç¡®æˆæœ¬ï¼ˆåŸºäºå®é™…tokenä½¿ç”¨ï¼‰
///
/// åŸºäºå®é™…çš„tokenç»Ÿè®¡è®¡ç®—ç²¾ç¡®æˆæœ¬
///
/// å‚æ•°ï¼š
/// - `provider_id`: ä¾›åº”å•†ID
/// - `model_id`: æ¨¡å‹ID
/// - `input_tokens`: è¾“å…¥tokenæ•°
/// - `output_tokens`: è¾“å‡ºtokenæ•°
/// - `cache_write_tokens`: å¯é€‰çš„ç¼“å­˜å†™å…¥tokenæ•°
/// - `cache_read_tokens`: å¯é€‰çš„ç¼“å­˜è¯»å–tokenæ•°
///
/// ç¤ºä¾‹ï¼š
/// ```ts
/// const breakdown = await invoke<CostBreakdown>('calculate_precise_cost', {
///   providerId: 'openai',
///   modelId: 'gpt-4o-mini',
///   inputTokens: 1000,
///   outputTokens: 500,
///   cacheReadTokens: 300
/// });
/// ```
#[tauri::command]
pub fn calculate_precise_cost(
    provider_id: String,
    model_id: String,
    input_tokens: usize,
    output_tokens: usize,
    cache_write_tokens: Option<usize>,
    cache_read_tokens: Option<usize>,
) -> Result<CostBreakdown, String> {
    let model = with_global_registry(|registry| {
        registry.get_provider(&provider_id)
            .and_then(|provider| provider.get_model_info(&model_id))
            .ok_or_else(|| format!("æœªæ‰¾åˆ°æ¨¡å‹: {} (ä¾›åº”å•†: {})", model_id, provider_id))
    })?;

    let breakdown = CostCalculator::calculate_openai(
        &model,
        input_tokens,
        output_tokens,
        cache_write_tokens.unwrap_or(0),
        cache_read_tokens.unwrap_or(0),
    );

    Ok(breakdown)
}

/// è·å–æ‰€æœ‰æ”¯æŒçš„ä¾›åº”å•†åˆ—è¡¨ (Phase 1 é‡æ„ - åŠ¨æ€ä¾›åº”å•†)
///
/// è¿”å›æ‰€æœ‰å·²æ³¨å†Œçš„AIä¾›åº”å•†ä¿¡æ¯ï¼Œæ”¯æŒæ’ä»¶åŒ–æ¶æ„
///
/// ç¤ºä¾‹ï¼š
/// ```ts
/// const providers = await invoke<ProviderInfo[]>('get_all_providers');
/// // [{ id: "deepseek", display_name: "DeepSeek AI", ... }]
/// ```
#[tauri::command]
pub fn get_all_providers() -> Result<Vec<ProviderInfo>, String> {
    // ä¾›åº”å•†å·²åœ¨åº”ç”¨å¯åŠ¨æ—¶æ³¨å†Œï¼ˆinit.rs:init_ai_providersï¼‰
    let providers = with_global_registry(|registry| {
        registry.get_all_providers()
    });
    
    Ok(providers)
}

/// è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ï¼ˆæ¥è‡ªæ‰€æœ‰ä¾›åº”å•†ï¼‰
///
/// ç¤ºä¾‹ï¼š
/// ```ts
/// const models = await invoke<ModelInfo[]>('get_all_models');
/// ```
#[tauri::command]
pub fn get_all_models() -> Result<Vec<ModelInfo>, String> {
    // ä¾›åº”å•†å·²åœ¨åº”ç”¨å¯åŠ¨æ—¶æ³¨å†Œï¼ˆinit.rs:init_ai_providersï¼‰
    let models = with_global_registry(|registry| {
        registry.get_all_models()
    });
    
    Ok(models)
}

/// æ ¹æ®æ¨¡å‹IDæŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†ä¿¡æ¯
///
/// ç¤ºä¾‹ï¼š
/// ```ts
/// const provider = await invoke<ProviderInfo | null>('find_provider_for_model', {
///   modelId: 'deepseek-chat'
/// });
/// ```
#[tauri::command]
pub fn find_provider_for_model(model_id: String) -> Result<Option<ProviderInfo>, String> {
    // ä¾›åº”å•†å·²åœ¨åº”ç”¨å¯åŠ¨æ—¶æ³¨å†Œï¼ˆinit.rs:init_ai_providersï¼‰
    let provider_info = with_global_registry(|registry| {
        registry.find_provider_for_model(&model_id)
            .map(|provider| provider.get_provider_info())
    });
    
    Ok(provider_info)
}

#[cfg(test)]
#[allow(clippy::unwrap_used, clippy::expect_used)]
mod tests {
    use super::*;
    use crate::services::ai::providers::register_all_providers;

    #[tokio::test]
    async fn test_get_provider_models() {
        // æµ‹è¯•ç¯å¢ƒï¼šæ‰‹åŠ¨æ³¨å†Œä¾›åº”å•†ï¼ˆç”Ÿäº§ç¯å¢ƒåœ¨ init.rs ä¸­è‡ªåŠ¨æ³¨å†Œï¼‰
        let _ = register_all_providers();
        
        let models = get_provider_models("openai".to_string());
        assert!(models.is_ok());
        let models = models.unwrap();
        assert!(!models.is_empty());

        // éªŒè¯è¿”å›çš„æ¨¡å‹ä¿¡æ¯å®Œæ•´
        let first_model = &models[0];
        assert!(!first_model.id.is_empty());
        assert!(!first_model.name.is_empty());
        assert_eq!(first_model.provider, "OpenAI");
        assert!(first_model.context_window > 0);
    }

    #[tokio::test]
    async fn test_get_model_info() {
        let _ = register_all_providers();
        
        let model = get_model_info("openai".to_string(), "gpt-4o-mini".to_string());
        assert!(model.is_ok());
        let model = model.unwrap();
        assert!(model.is_some());

        let model = model.unwrap();
        assert_eq!(model.id, "gpt-4o-mini");
        assert_eq!(model.provider, "OpenAI");
    }

    #[tokio::test]
    async fn test_estimate_translation_cost() {
        let _ = register_all_providers();
        
        let cost = estimate_translation_cost(
            "openai".to_string(),
            "gpt-4o-mini".to_string(),
            10000,
            Some(0.3),
        );

        assert!(cost.is_ok());
        let cost_value = cost.unwrap();
        assert!(cost_value > 0.0);
        assert!(cost_value < 1.0); // 10000å­—ç¬¦ä¸åº”è¶…è¿‡1ç¾å…ƒ
    }

    #[tokio::test]
    async fn test_calculate_precise_cost() {
        let _ = register_all_providers();
        
        let breakdown = calculate_precise_cost(
            "openai".to_string(),
            "gpt-4o-mini".to_string(),
            1000,
            500,
            None,
            Some(300),
        );

        assert!(breakdown.is_ok());
        let breakdown = breakdown.unwrap();
        assert_eq!(breakdown.input_tokens, 1000);
        assert_eq!(breakdown.output_tokens, 500);
        assert_eq!(breakdown.cache_read_tokens, 300);
        assert!(breakdown.total_cost > 0.0);
        assert!(breakdown.cache_savings > 0.0);
        assert!((breakdown.cache_hit_rate - 30.0).abs() < 0.1);
    }

    #[tokio::test]
    async fn test_invalid_cache_hit_rate() {
        let _ = register_all_providers();
        
        let cost = estimate_translation_cost(
            "openai".to_string(),
            "gpt-4o-mini".to_string(),
            10000,
            Some(1.5), // æ— æ•ˆï¼šè¶…è¿‡1.0
        );

        assert!(cost.is_err());
        assert!(cost.unwrap_err().contains("0.0-1.0"));
    }

    #[tokio::test]
    async fn test_nonexistent_model() {
        let _ = register_all_providers();
        
        let model = get_model_info("openai".to_string(), "nonexistent-model".to_string());
        assert!(model.is_ok());
        let model = model.unwrap();
        assert!(model.is_none());

        let cost = estimate_translation_cost(
            "openai".to_string(),
            "nonexistent-model".to_string(),
            10000,
            None,
        );
        assert!(cost.is_err());
        assert!(cost.unwrap_err().contains("æœªæ‰¾åˆ°æ¨¡å‹"));
    }
}
